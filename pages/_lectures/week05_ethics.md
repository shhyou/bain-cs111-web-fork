---
layout: module
title: Bias
type: ethics
draft: 0
num: 4
canvas_id: 1307440
lec_assignment: 1
canvas_title: Ethics Module 4 - Bias
due_date: 2023-10-16
readings:
   - title: "Racial Discrimination in Face Recognition Technology"
     author: "Alex Najibi for <em>Science in the News</em> at Harvard University."
     url: https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/
     notes: "(Note: while this is a general summary, many of the linked studies dive much deeper into the issues.)"
---

While AI has the potential to revolutionize work and recreation, its use can lead to unintentional bias and discrimination. One of the key issues with AI bias is that it can perpetuate existing inequalities and reinforce stereotypes. For example, biased AI algorithms used in hiring processes can lead to discrimination against certain groups based on race, gender, or other factors. Similarly, AI used in criminal justice systems can exacerbate existing biases and result in unfair treatment of certain individuals. The data the models are trained on can lead to sampling bias, confirmation bias, and selection bias, among others. If these biases are not identified and addressed, they can lead to inaccurate and unfair outcomes.

As AI continues to be integrated into various industries, it is important to consider the potential for bias and take steps to mitigate its impact. This includes evaluating the data used to train AI algorithms and ensuring that diverse perspectives are represented. It also involves being mindful of the potential for unintended consequences and actively working to address bias in AI systems. This module introduces students to the issue of bias via an interactive game, and the discussion applies studentsâ€™ observations about the game to a real-world case study of the harms of algorithmic bias.

* * *

## Part 1 - Complete the Pre-Discussion Activity

Everyone is required to complete this pre-discussion activity. Please read the article linked at the bottom of the page about _Racial Discrimination in Face Recognition Technology_ written by Alex Najibi for the _Science in the News_ project at Harvard University. While you are reading, consider the following questions:

* In what contexts have you interacted with facial recognition technology in the last few years?
* Do you know if your likeness is stored locally or on a server somewhere?
* Can you think of other situations where training data being selected via convenience may lead to bias in algorithmic outputs?

* * *

## Part 2 - Attend your Discussion Section

You should have registered for a discussion section this week by joining the appropriate Canvas group. Switching sessions is not allowed and attendance at your registered section is required and you must be in-attendance _for the entirety of the session_. For questions about discussion sections, please contact our Ethics Coordinator, Maya Blumovitz (<maya.blumovitz@u.northwestern.edu>) and CC Prof. Bain (<connor.bain@northwestern.edu>).

* * *

## Part 3 - Submit your Self Reflection

After attending your discussion section, be sure to return here to complete this self-reflection. Reflect on your discussion section (200-300 words): What strategies could be used to audit systems for bias? Was there anything you particularly liked or disliked about the discussion, or any other feedback? Make sure to submit your thoughts either via text entry or file upload in Canvas.